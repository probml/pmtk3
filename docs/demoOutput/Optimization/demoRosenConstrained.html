
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>demoRosenConstrained</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-04-29"><meta name="m-file" content="demoRosenConstrained"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><pre class="codeinput"><span class="keyword">function</span> demoRosenConstrained()

<span class="comment">% minimize 2d rosenbrock st x1^2 + x^2 &lt;= 1</span>
<span class="comment">% Example from p1-8 of Mathworks Optimization Toolbox manual</span>

xstart = [-1 2];
 <span class="comment">% Hessian is ignored by quasi-Newton so we use interior point</span>
opts = optimset(<span class="string">'DerivativeCheck'</span>, <span class="string">'on'</span>, <span class="string">'Display'</span>, <span class="string">'off'</span>, <span class="string">'GradObj'</span>, <span class="string">'on'</span>);

<span class="comment">% basic usage - numerical gradient for constraints</span>
opts = optimset(opts, <span class="string">'GradConstr'</span>, []);
[x fval exitflag output] = fmincon(@rosen2d, xstart, [], [], [], [], [], [], @constr, opts);
fprintf(<span class="string">'fcount = %d\n\n\n'</span>, output.funcCount)

<span class="comment">% analytic gradient for constraints</span>
opts = optimset(opts, <span class="string">'GradConstr'</span>, <span class="string">'on'</span>);
[x fval exitflag output] = fmincon(@rosen2d, xstart, [], [], [], [], [], [], @constrgrad, opts);
fprintf(<span class="string">'fcount = %d\n\n\n'</span>, output.funcCount)


<span class="comment">% analytic Hessian for objective and constraints (see p4-17, p11-42 for syntax)</span>
<span class="comment">% If you don't use interior-point, the method defaults to quasi-Newton</span>
<span class="comment">% which obviously does not need a Hessian</span>
opts = optimset(opts,  <span class="string">'GradConstr'</span>, <span class="string">'on'</span>, <span class="string">'Algorithm'</span>, <span class="string">'interior-point'</span>, <span class="keyword">...</span>
  <span class="string">'Hessian'</span>,<span class="string">'user-supplied'</span>,<span class="string">'HessFcn'</span>,@hess);
[x fval exitflag output] = fmincon(@rosen2d, xstart, [], [], [], [], [], [], @constrgrad, opts);
fprintf(<span class="string">'fcount = %d\n\n\n'</span>, output.funcCount)

<span class="keyword">end</span>

<span class="keyword">function</span> [c, ceq] = constr(x)
c = x(1)^2 + x(2)^2 - 1;
ceq = [];
<span class="keyword">end</span>

<span class="keyword">function</span> [c, ceq, cgrad, ceqgrad] = constrgrad(x)
c = x(1)^2 + x(2)^2 - 1;
ceq = 0;
cgrad = 2*x(:); <span class="comment">% column j is gradient wrt j'th constraint</span>
ceqgrad = zeros(2,1);
<span class="keyword">end</span>

<span class="keyword">function</span> H = hess(x, lambda)
[fx,gx,Hx] = rosen2d(x(:)');
H = Hx + lambda.ineqnonlin*2*eye(length(x));
<span class="keyword">end</span>
</pre><pre class="codeoutput">Warning: Trust-region-reflective algorithm
does not solve this type of problem, using
active-set algorithm. You could also try
the interior-point or sqp algorithms: set
the Algorithm option to 'interior-point' or
'sqp' and rerun. For more help, see
Choosing the Algorithm in the
documentation. 
Objective function derivative:
Maximum relative discrepancy between derivatives = 1.43051e-008
fcount = 126


Warning: Trust-region-reflective algorithm
does not solve this type of problem, using
active-set algorithm. You could also try
the interior-point or sqp algorithms: set
the Algorithm option to 'interior-point' or
'sqp' and rerun. For more help, see
Choosing the Algorithm in the
documentation. 
Objective function derivative:
Maximum relative discrepancy between derivatives = 1.43051e-008
Nonlinear inequality constraint derivatives:
Maximum relative discrepancy between derivatives = 7.45058e-009
Nonlinear equality constraint derivative:
Maximum relative discrepancy between derivatives = 0
fcount = 73


Objective function derivatives:
Maximum relative discrepancy between derivatives = 1.48535e-008
Nonlinear inequality constraint derivatives:
Maximum relative discrepancy between derivatives = 7.45058e-009
Nonlinear equality constraint derivatives:
Maximum relative discrepancy between derivatives = 0
fcount = 32


</pre><p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
function demoRosenConstrained()

% minimize 2d rosenbrock st x1^2 + x^2 <= 1
% Example from p1-8 of Mathworks Optimization Toolbox manual

xstart = [-1 2];
 % Hessian is ignored by quasi-Newton so we use interior point
opts = optimset('DerivativeCheck', 'on', 'Display', 'off', 'GradObj', 'on');

% basic usage - numerical gradient for constraints
opts = optimset(opts, 'GradConstr', []);
[x fval exitflag output] = fmincon(@rosen2d, xstart, [], [], [], [], [], [], @constr, opts);
fprintf('fcount = %d\n\n\n', output.funcCount)

% analytic gradient for constraints
opts = optimset(opts, 'GradConstr', 'on');
[x fval exitflag output] = fmincon(@rosen2d, xstart, [], [], [], [], [], [], @constrgrad, opts);
fprintf('fcount = %d\n\n\n', output.funcCount)


% analytic Hessian for objective and constraints (see p4-17, p11-42 for syntax)
% If you don't use interior-point, the method defaults to quasi-Newton
% which obviously does not need a Hessian
opts = optimset(opts,  'GradConstr', 'on', 'Algorithm', 'interior-point', ...
  'Hessian','user-supplied','HessFcn',@hess);
[x fval exitflag output] = fmincon(@rosen2d, xstart, [], [], [], [], [], [], @constrgrad, opts);
fprintf('fcount = %d\n\n\n', output.funcCount)

end

function [c, ceq] = constr(x)
c = x(1)^2 + x(2)^2 - 1;
ceq = [];
end

function [c, ceq, cgrad, ceqgrad] = constrgrad(x)
c = x(1)^2 + x(2)^2 - 1;
ceq = 0;
cgrad = 2*x(:); % column j is gradient wrt j'th constraint
ceqgrad = zeros(2,1);
end

function H = hess(x, lambda)
[fx,gx,Hx] = rosen2d(x(:)');
H = Hx + lambda.ineqnonlin*2*eye(length(x));
end

##### SOURCE END #####
--></body></html>