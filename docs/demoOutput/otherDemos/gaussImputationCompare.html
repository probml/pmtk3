
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>Compare the results of imputation on a MVN using three imputation methods: EM, ICM, and Gibbs</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2010-07-27"><meta name="m-file" content="gaussImputationCompare"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h1>Compare the results of imputation on a MVN using three imputation methods: EM, ICM, and Gibbs</h1><!--introduction--><p>PMTKauthor Cody Severinski</p><!--/introduction--><pre class="codeinput">setSeed(1);
d = 10; n = 100;
mu = randn(d,1); Sigma = randpd(d);

pcMissing = 0.2;
model = struct(<span class="string">'mu'</span>, mu, <span class="string">'Sigma'</span>, Sigma);
Xfull = gaussSample(model, n);
missing = rand(n,d) &lt; pcMissing;

Xmiss = Xfull;
Xmiss(missing) = NaN;
Xhid = Xfull;
Xhid(~missing) = NaN;
verb = true;

<span class="comment">% first EM</span>
fprintf(<span class="string">'EM First\n'</span>)
[model, LLtrace{1}] = gaussMissingFitEm(Xmiss, <span class="string">'verbose'</span>, verb, <span class="string">'maxIter'</span>, 500);
muHat{1} = model.mu;
SigmaHat{1} = model.Sigma;

<span class="comment">% second ICM</span>
fprintf(<span class="string">'Now ICM\n'</span>)
[model, LLtrace{2}] = gaussMissingFitICM(Xmiss, <span class="string">'verbose'</span>, verb);
muHat{2} = model.mu;
SigmaHat{2} = model.Sigma;
<span class="comment">% third Gibbs</span>
fprintf(<span class="string">'Now Gibbs\n'</span>)
[model, dataSamples, LLtrace{3}] = gaussMissingFitGibbs(Xmiss, <span class="string">'mu0'</span>, nanmeanPMTK(Xmiss), <span class="string">'Lambda0'</span>, diag(nanvarPMTK(Xmiss)), <span class="string">'k0'</span>, 0.01, <span class="string">'dof'</span>, d + 2, <span class="string">'verbose'</span>, verb);
muSamples = model.mu; SigmaSamples = model.Sigma;
muHat{3} = mean(muSamples);
SigmaHat{3} = mean(SigmaSamples,3);

method = {<span class="string">'EM'</span>, <span class="string">'ICM'</span>, <span class="string">'Gibbs'</span>};
<span class="comment">% Print out some information</span>
fprintf(<span class="string">'True mean:\t\t %s\n'</span>, mat2str(rowvec(mu),2))
<span class="keyword">for</span> m=1:length(method)
  fprintf(<span class="string">'Mean for method %s:\t %s\n'</span>, method{m}, mat2str(rowvec(muHat{m}),2))
<span class="comment">%  fprintf('Variance for method %s: %s\n', method{m}, mat2str(SigmaHat{m}))</span>
<span class="keyword">end</span>

plotOpts = {<span class="string">'b'</span>,<span class="string">'g'</span>,<span class="string">'r'</span>};
<span class="comment">% Plot the estimated means</span>
figure(); subplot(3,3,1:3); hold <span class="string">on</span>;
<span class="keyword">for</span> m=1:length(method)
  plot(1:d, rowvec(muHat{m}), plotOpts{m})
<span class="keyword">end</span>
xlabel(<span class="string">'Dimension'</span>); ylabel(<span class="string">'Mean'</span>); <span class="comment">%legend(method, 'Location', 'Best');</span>
title(<span class="string">'Mean estimate for each method'</span>);

<span class="comment">% Plot the difference in the means</span>
subplot(3,3,4:6); hold <span class="string">on</span>;
<span class="keyword">for</span> m=1:length(method)
  plot(1:d, rowvec(muHat{m}) - rowvec(mu), plotOpts{m});
<span class="keyword">end</span>
xlabel(<span class="string">'Dimension'</span>); ylabel(<span class="string">'Difference in Mean'</span>);
title(<span class="string">'Difference in mean estimate from truth'</span>);

<span class="comment">% Plot the trace of log likelihood over iterations for all methods</span>
a = zeros(3,4);
<span class="keyword">for</span> m=1:length(method)
  subplot(3,3,6+m);
  h{m} = plot(LLtrace{m}, plotOpts{m});
  a(m,:) = axis;
  set(gca,<span class="string">'XTickLabel'</span>,num2str(get(gca,<span class="string">'XTick'</span>).'));
  set(gca,<span class="string">'YTickLabel'</span>,num2str(get(gca,<span class="string">'YTick'</span>).'))
<span class="keyword">end</span>

<span class="comment">% Adjust the axis -- does not improve visualization</span>
<span class="comment">%ymin = min(a(:,3)); ymax = max(a(:,4));</span>
<span class="comment">%a(:,3) = ymin; a(:,4) = ymax;</span>
<span class="comment">%for m=1:length(method)</span>
<span class="comment">%  subplot(3,3,6+m);</span>
<span class="comment">%  axis(a(m,:));</span>
<span class="comment">%end</span>
<span class="comment">%suplabel('Iteration', 'x', [.075 .1 .85 .85]); suplabel('Log likelihood', 'y', [.1 .075 .85 .85/3]);</span>
<span class="comment">%title('Log-likelihood trace over iterations');</span>

<span class="comment">% Place one overall legend</span>
subplot(3,3,[1:3]);
L = legend(method);
set(L, <span class="string">'position'</span>, [0.1, 0.02, 0.8, 0.03]);
set(L, <span class="string">'fontsize'</span>, 8);
set(L, <span class="string">'orientation'</span>, <span class="string">'horizontal'</span>);
</pre><pre class="codeoutput">EM First
1	 loglik: -2040.36
2	 loglik: -2000.04
3	 loglik: -1981.7
4	 loglik: -1971.15
5	 loglik: -1964.43
6	 loglik: -1959.87
7	 loglik: -1956.62
8	 loglik: -1954.25
9	 loglik: -1952.46
10	 loglik: -1951.08
11	 loglik: -1950
12	 loglik: -1949.12
13	 loglik: -1948.41
14	 loglik: -1947.82
15	 loglik: -1947.33
16	 loglik: -1946.91
17	 loglik: -1946.55
18	 loglik: -1946.24
19	 loglik: -1945.98
20	 loglik: -1945.74
21	 loglik: -1945.53
22	 loglik: -1945.35
Now ICM
1: LL = -2265.939
2: LL = -2070.704
3: LL = -1975.069
4: LL = -1928.008
5: LL = -1903.059
6: LL = -1880.911
7: LL = -1856.220
8: LL = -1837.670
9: LL = -1829.118
10: LL = -1824.069
11: LL = -1819.984
12: LL = -1816.417
13: LL = -1813.246
14: LL = -1810.407
15: LL = -1807.860
16: LL = -1805.572
17: LL = -1803.516
18: LL = -1801.669
19: LL = -1800.008
20: LL = -1798.516
21: LL = -1797.175
22: LL = -1795.970
23: LL = -1794.885
24: LL = -1793.909
25: LL = -1793.029
26: LL = -1792.235
27: LL = -1791.518
28: LL = -1790.869
Now Gibbs
1: LL = -2382.569
2: LL = -2384.699
3: LL = -2390.112
4: LL = -2411.647
5: LL = -2393.269
6: LL = -2405.911
7: LL = -2374.137
8: LL = -2380.184
9: LL = -2372.027
10: LL = -2378.268
11: LL = -2380.055
12: LL = -2363.108
13: LL = -2370.695
14: LL = -2388.959
15: LL = -2392.561
16: LL = -2395.237
17: LL = -2402.284
18: LL = -2398.229
19: LL = -2387.929
20: LL = -2376.237
21: LL = -2383.681
22: LL = -2409.986
23: LL = -2407.200
24: LL = -2392.743
25: LL = -2386.033
26: LL = -2385.356
27: LL = -2393.128
28: LL = -2375.760
29: LL = -2415.974
30: LL = -2383.035
31: LL = -2374.517
32: LL = -2397.362
33: LL = -2407.145
34: LL = -2382.284
35: LL = -2389.315
36: LL = -2414.622
37: LL = -2383.010
38: LL = -2371.298
39: LL = -2373.227
40: LL = -2384.524
41: LL = -2355.455
42: LL = -2347.445
43: LL = -2353.304
44: LL = -2359.367
45: LL = -2361.859
46: LL = -2391.747
47: LL = -2396.163
48: LL = -2406.553
49: LL = -2381.068
50: LL = -2396.480
51: LL = -2386.916
52: LL = -2391.551
53: LL = -2373.605
54: LL = -2374.568
55: LL = -2375.096
56: LL = -2392.818
57: LL = -2387.197
58: LL = -2380.032
59: LL = -2405.828
60: LL = -2398.720
61: LL = -2330.034
62: LL = -2344.739
63: LL = -2403.597
64: LL = -2443.295
65: LL = -2436.287
66: LL = -2391.030
67: LL = -2377.154
68: LL = -2388.128
69: LL = -2373.566
70: LL = -2402.762
71: LL = -2424.648
72: LL = -2414.793
73: LL = -2410.287
74: LL = -2386.596
75: LL = -2389.385
76: LL = -2369.074
77: LL = -2396.329
78: LL = -2400.169
79: LL = -2368.880
80: LL = -2404.947
81: LL = -2420.873
82: LL = -2366.887
83: LL = -2378.960
84: LL = -2336.342
85: LL = -2384.972
86: LL = -2394.311
87: LL = -2384.422
88: LL = -2401.800
89: LL = -2395.273
90: LL = -2422.549
91: LL = -2394.559
92: LL = -2381.928
93: LL = -2353.488
94: LL = -2366.066
95: LL = -2366.985
96: LL = -2394.392
97: LL = -2366.206
98: LL = -2368.283
99: LL = -2404.885
100: LL = -2404.132
101: LL = -2388.801
102: LL = -2410.573
103: LL = -2366.366
104: LL = -2414.978
105: LL = -2401.326
106: LL = -2396.607
107: LL = -2406.557
108: LL = -2384.320
109: LL = -2388.040
110: LL = -2403.001
111: LL = -2397.068
112: LL = -2407.499
113: LL = -2436.216
114: LL = -2450.590
115: LL = -2385.397
116: LL = -2384.279
117: LL = -2390.272
118: LL = -2368.620
119: LL = -2349.754
120: LL = -2367.294
121: LL = -2377.159
122: LL = -2387.775
123: LL = -2382.777
124: LL = -2381.005
125: LL = -2391.269
126: LL = -2410.464
127: LL = -2372.030
128: LL = -2368.849
129: LL = -2375.154
130: LL = -2384.456
131: LL = -2370.818
132: LL = -2390.893
133: LL = -2389.271
134: LL = -2387.822
135: LL = -2395.628
136: LL = -2370.868
137: LL = -2386.067
138: LL = -2357.543
139: LL = -2345.399
140: LL = -2371.386
141: LL = -2390.530
142: LL = -2394.963
143: LL = -2408.529
144: LL = -2410.762
145: LL = -2392.952
146: LL = -2412.690
147: LL = -2380.580
148: LL = -2357.824
149: LL = -2387.880
150: LL = -2399.599
151: LL = -2374.020
152: LL = -2363.578
153: LL = -2362.760
154: LL = -2366.562
155: LL = -2368.260
156: LL = -2357.111
157: LL = -2348.078
158: LL = -2378.987
159: LL = -2381.150
160: LL = -2387.367
161: LL = -2384.762
162: LL = -2387.741
163: LL = -2399.350
164: LL = -2398.345
165: LL = -2379.050
166: LL = -2379.431
167: LL = -2385.464
168: LL = -2382.379
169: LL = -2375.091
170: LL = -2354.489
171: LL = -2368.033
172: LL = -2337.840
173: LL = -2364.811
174: LL = -2385.619
175: LL = -2407.935
176: LL = -2379.699
177: LL = -2380.271
178: LL = -2375.855
179: LL = -2392.063
180: LL = -2402.508
181: LL = -2399.857
182: LL = -2378.787
183: LL = -2393.119
184: LL = -2373.828
185: LL = -2345.870
186: LL = -2336.745
187: LL = -2358.128
188: LL = -2359.462
189: LL = -2394.894
190: LL = -2376.536
191: LL = -2370.026
192: LL = -2354.520
193: LL = -2350.078
194: LL = -2374.283
195: LL = -2393.974
196: LL = -2396.271
197: LL = -2408.742
198: LL = -2373.334
199: LL = -2382.834
200: LL = -2371.901
201: LL = -2405.232
202: LL = -2423.823
203: LL = -2404.802
204: LL = -2367.832
205: LL = -2381.856
206: LL = -2381.345
207: LL = -2397.159
208: LL = -2394.803
209: LL = -2405.526
210: LL = -2402.303
211: LL = -2396.126
212: LL = -2382.551
213: LL = -2359.860
214: LL = -2349.176
215: LL = -2370.164
216: LL = -2383.503
217: LL = -2367.171
218: LL = -2369.361
219: LL = -2403.706
220: LL = -2396.016
221: LL = -2366.233
222: LL = -2367.527
223: LL = -2353.807
224: LL = -2373.038
225: LL = -2377.553
226: LL = -2377.407
227: LL = -2375.668
228: LL = -2359.449
229: LL = -2407.585
230: LL = -2385.025
231: LL = -2386.968
232: LL = -2368.132
233: LL = -2375.155
234: LL = -2347.305
235: LL = -2367.124
236: LL = -2369.657
237: LL = -2388.092
238: LL = -2380.121
239: LL = -2382.123
240: LL = -2384.718
241: LL = -2385.841
242: LL = -2366.276
243: LL = -2376.376
244: LL = -2373.549
245: LL = -2380.310
246: LL = -2371.590
247: LL = -2389.895
248: LL = -2403.940
249: LL = -2433.411
250: LL = -2405.597
251: LL = -2410.888
252: LL = -2374.094
253: LL = -2395.996
254: LL = -2407.718
255: LL = -2389.011
256: LL = -2376.678
257: LL = -2365.122
258: LL = -2364.081
259: LL = -2363.557
260: LL = -2396.089
261: LL = -2395.673
262: LL = -2390.614
263: LL = -2398.937
264: LL = -2404.316
265: LL = -2386.107
266: LL = -2378.898
267: LL = -2380.415
268: LL = -2398.903
269: LL = -2366.667
270: LL = -2374.382
271: LL = -2387.107
272: LL = -2406.844
273: LL = -2385.714
274: LL = -2363.219
275: LL = -2371.491
276: LL = -2380.327
277: LL = -2392.704
278: LL = -2420.105
279: LL = -2400.270
280: LL = -2375.337
281: LL = -2392.215
282: LL = -2392.012
283: LL = -2396.025
284: LL = -2371.427
285: LL = -2392.229
286: LL = -2388.612
287: LL = -2363.491
288: LL = -2388.290
289: LL = -2385.130
290: LL = -2386.147
291: LL = -2372.590
292: LL = -2369.982
293: LL = -2386.023
294: LL = -2408.881
295: LL = -2391.824
296: LL = -2377.214
297: LL = -2381.562
298: LL = -2375.619
299: LL = -2359.839
300: LL = -2364.882
301: LL = -2385.038
302: LL = -2396.410
303: LL = -2397.522
304: LL = -2380.448
305: LL = -2363.393
306: LL = -2384.600
307: LL = -2440.431
308: LL = -2414.191
309: LL = -2412.761
310: LL = -2394.647
311: LL = -2409.603
312: LL = -2394.085
313: LL = -2401.125
314: LL = -2385.864
315: LL = -2414.560
316: LL = -2436.784
317: LL = -2403.533
318: LL = -2387.988
319: LL = -2359.762
320: LL = -2349.019
321: LL = -2381.382
322: LL = -2365.967
323: LL = -2360.694
324: LL = -2378.573
325: LL = -2372.393
326: LL = -2386.992
327: LL = -2368.906
328: LL = -2396.534
329: LL = -2371.180
330: LL = -2405.713
331: LL = -2392.070
332: LL = -2393.926
333: LL = -2368.977
334: LL = -2372.565
335: LL = -2376.830
336: LL = -2355.223
337: LL = -2379.688
338: LL = -2375.528
339: LL = -2371.793
340: LL = -2373.203
341: LL = -2357.473
342: LL = -2342.336
343: LL = -2357.429
344: LL = -2379.348
345: LL = -2358.061
346: LL = -2366.157
347: LL = -2387.475
348: LL = -2383.040
349: LL = -2363.877
350: LL = -2393.050
351: LL = -2394.644
352: LL = -2398.143
353: LL = -2351.366
354: LL = -2343.280
355: LL = -2400.205
356: LL = -2397.035
357: LL = -2392.989
358: LL = -2403.096
359: LL = -2423.633
360: LL = -2419.215
361: LL = -2384.217
362: LL = -2397.257
363: LL = -2387.466
364: LL = -2384.484
365: LL = -2408.562
366: LL = -2368.537
367: LL = -2359.420
368: LL = -2364.255
369: LL = -2394.318
370: LL = -2383.710
371: LL = -2368.158
372: LL = -2366.351
373: LL = -2365.405
374: LL = -2362.521
375: LL = -2356.930
376: LL = -2339.116
377: LL = -2359.997
378: LL = -2390.949
379: LL = -2399.330
380: LL = -2394.295
381: LL = -2414.746
382: LL = -2376.376
383: LL = -2383.720
384: LL = -2379.039
385: LL = -2392.652
386: LL = -2396.649
387: LL = -2409.945
388: LL = -2386.209
389: LL = -2379.381
390: LL = -2353.781
391: LL = -2355.302
392: LL = -2353.538
393: LL = -2355.028
394: LL = -2359.789
395: LL = -2384.913
396: LL = -2343.372
397: LL = -2333.121
398: LL = -2388.626
399: LL = -2400.890
400: LL = -2377.783
401: LL = -2374.933
402: LL = -2350.047
403: LL = -2354.263
404: LL = -2410.687
405: LL = -2381.610
406: LL = -2400.618
407: LL = -2397.841
408: LL = -2412.194
409: LL = -2410.135
410: LL = -2413.936
411: LL = -2370.450
412: LL = -2382.618
413: LL = -2395.586
414: LL = -2396.257
415: LL = -2367.565
416: LL = -2360.706
417: LL = -2388.143
418: LL = -2381.116
419: LL = -2381.866
420: LL = -2406.330
421: LL = -2383.578
422: LL = -2383.931
423: LL = -2369.261
424: LL = -2390.349
425: LL = -2397.052
426: LL = -2406.365
427: LL = -2390.835
428: LL = -2411.568
429: LL = -2406.324
430: LL = -2403.646
431: LL = -2377.317
432: LL = -2404.060
433: LL = -2380.603
434: LL = -2389.162
435: LL = -2389.004
436: LL = -2408.127
437: LL = -2374.216
438: LL = -2389.908
439: LL = -2375.479
440: LL = -2387.135
441: LL = -2390.674
442: LL = -2392.334
443: LL = -2368.167
444: LL = -2391.222
445: LL = -2408.056
446: LL = -2426.558
447: LL = -2381.186
448: LL = -2388.775
449: LL = -2424.628
450: LL = -2380.980
451: LL = -2408.375
452: LL = -2385.511
453: LL = -2376.276
454: LL = -2401.647
455: LL = -2387.873
456: LL = -2384.610
457: LL = -2384.943
458: LL = -2378.774
459: LL = -2407.175
460: LL = -2405.127
461: LL = -2407.909
462: LL = -2387.312
463: LL = -2390.771
464: LL = -2394.828
465: LL = -2371.199
466: LL = -2412.467
467: LL = -2394.642
468: LL = -2381.497
469: LL = -2357.371
470: LL = -2373.100
471: LL = -2397.586
472: LL = -2380.514
473: LL = -2405.404
474: LL = -2394.229
475: LL = -2375.109
476: LL = -2377.103
477: LL = -2396.195
478: LL = -2382.627
479: LL = -2404.837
480: LL = -2377.465
481: LL = -2358.422
482: LL = -2400.578
483: LL = -2402.710
484: LL = -2420.382
485: LL = -2414.561
486: LL = -2405.440
487: LL = -2425.560
488: LL = -2377.889
489: LL = -2380.320
490: LL = -2409.150
491: LL = -2372.245
492: LL = -2377.870
493: LL = -2389.071
494: LL = -2384.335
495: LL = -2402.341
496: LL = -2394.648
497: LL = -2392.194
498: LL = -2391.164
499: LL = -2385.921
500: LL = -2384.908
True mean:		 [0.86 0.094 -0.85 0.87 -0.44 -0.43 -1.1 0.4 -0.96 0.17]
Mean for method EM:	 [0.69 -0.037 -0.97 0.73 -0.097 -0.69 -1.3 0.39 -1.2 -0.13]
Mean for method ICM:	 [0.72 0.0069 -1 0.74 -0.1 -0.69 -1.4 0.39 -1.2 -0.12]
Mean for method Gibbs:	 [0.66 -0.044 -0.93 0.72 -0.045 -0.68 -1.2 0.39 -1.2 -0.12]
</pre><img vspace="5" hspace="5" src="gaussImputationCompare_01.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
%% Compare the results of imputation on a MVN using three imputation methods: EM, ICM, and Gibbs
% PMTKauthor Cody Severinski
%%
setSeed(1);
d = 10; n = 100;
mu = randn(d,1); Sigma = randpd(d);

pcMissing = 0.2;
model = struct('mu', mu, 'Sigma', Sigma);
Xfull = gaussSample(model, n);
missing = rand(n,d) < pcMissing;

Xmiss = Xfull;
Xmiss(missing) = NaN;
Xhid = Xfull;
Xhid(~missing) = NaN;
verb = true;

% first EM
fprintf('EM First\n')
[model, LLtrace{1}] = gaussMissingFitEm(Xmiss, 'verbose', verb, 'maxIter', 500);
muHat{1} = model.mu;
SigmaHat{1} = model.Sigma; 

% second ICM
fprintf('Now ICM\n')
[model, LLtrace{2}] = gaussMissingFitICM(Xmiss, 'verbose', verb);
muHat{2} = model.mu;
SigmaHat{2} = model.Sigma; 
% third Gibbs
fprintf('Now Gibbs\n')
[model, dataSamples, LLtrace{3}] = gaussMissingFitGibbs(Xmiss, 'mu0', nanmeanPMTK(Xmiss), 'Lambda0', diag(nanvarPMTK(Xmiss)), 'k0', 0.01, 'dof', d + 2, 'verbose', verb);
muSamples = model.mu; SigmaSamples = model.Sigma; 
muHat{3} = mean(muSamples);
SigmaHat{3} = mean(SigmaSamples,3);

method = {'EM', 'ICM', 'Gibbs'};
% Print out some information
fprintf('True mean:\t\t %s\n', mat2str(rowvec(mu),2))
for m=1:length(method)
  fprintf('Mean for method %s:\t %s\n', method{m}, mat2str(rowvec(muHat{m}),2))
%  fprintf('Variance for method %s: %s\n', method{m}, mat2str(SigmaHat{m}))
end

plotOpts = {'b','g','r'};
% Plot the estimated means
figure(); subplot(3,3,1:3); hold on;
for m=1:length(method)
  plot(1:d, rowvec(muHat{m}), plotOpts{m})
end
xlabel('Dimension'); ylabel('Mean'); %legend(method, 'Location', 'Best');
title('Mean estimate for each method');

% Plot the difference in the means
subplot(3,3,4:6); hold on;
for m=1:length(method)
  plot(1:d, rowvec(muHat{m}) - rowvec(mu), plotOpts{m});
end
xlabel('Dimension'); ylabel('Difference in Mean');
title('Difference in mean estimate from truth');

% Plot the trace of log likelihood over iterations for all methods
a = zeros(3,4);
for m=1:length(method)
  subplot(3,3,6+m);
  h{m} = plot(LLtrace{m}, plotOpts{m});
  a(m,:) = axis;
  set(gca,'XTickLabel',num2str(get(gca,'XTick').'));
  set(gca,'YTickLabel',num2str(get(gca,'YTick').'))
end

% Adjust the axis REPLACE_WITH_DASH_DASH does not improve visualization
%ymin = min(a(:,3)); ymax = max(a(:,4));
%a(:,3) = ymin; a(:,4) = ymax;
%for m=1:length(method)
%  subplot(3,3,6+m);
%  axis(a(m,:));
%end
%suplabel('Iteration', 'x', [.075 .1 .85 .85]); suplabel('Log likelihood', 'y', [.1 .075 .85 .85/3]);
%title('Log-likelihood trace over iterations');

% Place one overall legend
subplot(3,3,[1:3]);
L = legend(method);
set(L, 'position', [0.1, 0.02, 0.8, 0.03]);
set(L, 'fontsize', 8);
set(L, 'orientation', 'horizontal');

##### SOURCE END #####
--></body></html>